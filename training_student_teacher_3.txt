2024-05-02 20:37:58.588755: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-05-02 20:37:58.588830: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-05-02 20:37:58.590052: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-05-02 20:37:58.596289: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-02 20:37:59.365491: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-02 20:38:00.168693: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-05-02 20:38:00.204939: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-05-02 20:38:00.205175: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-05-02 20:38:00.461125: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-05-02 20:38:00.461373: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-05-02 20:38:00.461560: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-05-02 20:38:00.552429: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-05-02 20:38:00.552654: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-05-02 20:38:00.552846: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-05-02 20:38:00.552994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5484 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1
Num GPUs Available:  1
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 112, 112, 3)]     0         
                                                                 
 conv2d (Conv2D)             (None, 55, 55, 24)        672       
                                                                 
 batch_normalization (Batch  (None, 55, 55, 24)        96        
 Normalization)                                                  
                                                                 
 activation (Activation)     (None, 55, 55, 24)        0         
                                                                 
 linear_bottleneck_block (L  (None, 28, 28, 24)        1368      
 inearBottleneckBlock)                                           
                                                                 
 linear_bottleneck_block_1   (None, 28, 28, 24)        1368      
 (LinearBottleneckBlock)                                         
                                                                 
 linear_bottleneck_block_2   (None, 28, 28, 24)        1368      
 (LinearBottleneckBlock)                                         
                                                                 
 linear_bottleneck_block_3   (None, 28, 28, 24)        1368      
 (LinearBottleneckBlock)                                         
                                                                 
 linear_bottleneck_block_4   (None, 14, 14, 96)        4464      
 (LinearBottleneckBlock)                                         
                                                                 
 linear_bottleneck_block_5   (None, 14, 14, 96)        12384     
 (LinearBottleneckBlock)                                         
                                                                 
 linear_bottleneck_block_6   (None, 14, 14, 96)        12384     
 (LinearBottleneckBlock)                                         
                                                                 
 linear_bottleneck_block_7   (None, 14, 14, 96)        12384     
 (LinearBottleneckBlock)                                         
                                                                 
 linear_bottleneck_block_8   (None, 7, 7, 168)         20664     
 (LinearBottleneckBlock)                                         
                                                                 
 linear_bottleneck_block_9   (None, 7, 7, 168)         33768     
 (LinearBottleneckBlock)                                         
                                                                 
 linear_bottleneck_block_10  (None, 7, 7, 168)         33768     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_11  (None, 4, 4, 192)         38256     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_12  (None, 4, 4, 384)         83712     
  (LinearBottleneckBlock)                                        
                                                                 
 global_average_pooling2d (  (None, 384)               0         
 GlobalAveragePooling2D)                                         
                                                                 
 dropout (Dropout)           (None, 384)               0         
                                                                 
 output_1 (Dense)            (None, 200)               77000     
                                                                 
 output_2 (TempatureSoftmax  (None, 200)               0         
 ActivationLayer)                                                
                                                                 
=================================================================
Total params: 335024 (1.28 MB)
Trainable params: 326336 (1.24 MB)
Non-trainable params: 8688 (33.94 KB)
_________________________________________________________________
Epoch 1/100
2024-05-02 20:41:05.429604: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904
2024-05-02 20:41:07.018519: I external/local_xla/xla/service/service.cc:168] XLA service 0x55a46fdf28e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-05-02 20:41:07.018585: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1060 6GB, Compute Capability 6.1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1714678867.074925  860116 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
782/782 - 91s - loss: 4.6948 - output_1_loss: 4.5082 - output_2_loss: 0.0622 - output_1_Top-1 Accuracy: 0.1010 - output_1_Top-5 Accuracy: 0.2601 - val_loss: 3.8368 - val_output_1_loss: 3.6640 - val_output_2_loss: 0.0576 - val_output_1_Top-1 Accuracy: 0.1884 - val_output_1_Top-5 Accuracy: 0.4372 - lr: 0.1000 - 91s/epoch - 116ms/step
Epoch 2/100
782/782 - 73s - loss: 3.6812 - output_1_loss: 3.4842 - output_2_loss: 0.0657 - output_1_Top-1 Accuracy: 0.2189 - output_1_Top-5 Accuracy: 0.4758 - val_loss: 3.4411 - val_output_1_loss: 3.2657 - val_output_2_loss: 0.0585 - val_output_1_Top-1 Accuracy: 0.2574 - val_output_1_Top-5 Accuracy: 0.5179 - lr: 0.1000 - 73s/epoch - 93ms/step
Epoch 3/100
782/782 - 72s - loss: 3.3941 - output_1_loss: 3.1911 - output_2_loss: 0.0677 - output_1_Top-1 Accuracy: 0.2700 - output_1_Top-5 Accuracy: 0.5412 - val_loss: 3.4010 - val_output_1_loss: 3.2226 - val_output_2_loss: 0.0595 - val_output_1_Top-1 Accuracy: 0.2578 - val_output_1_Top-5 Accuracy: 0.5334 - lr: 0.1000 - 72s/epoch - 92ms/step
Epoch 4/100
782/782 - 73s - loss: 3.2315 - output_1_loss: 3.0240 - output_2_loss: 0.0692 - output_1_Top-1 Accuracy: 0.2977 - output_1_Top-5 Accuracy: 0.5762 - val_loss: 3.3764 - val_output_1_loss: 3.1902 - val_output_2_loss: 0.0621 - val_output_1_Top-1 Accuracy: 0.2745 - val_output_1_Top-5 Accuracy: 0.5409 - lr: 0.1000 - 73s/epoch - 93ms/step
Epoch 5/100
782/782 - 72s - loss: 3.1172 - output_1_loss: 2.9067 - output_2_loss: 0.0702 - output_1_Top-1 Accuracy: 0.3216 - output_1_Top-5 Accuracy: 0.6004 - val_loss: 3.4318 - val_output_1_loss: 3.2514 - val_output_2_loss: 0.0601 - val_output_1_Top-1 Accuracy: 0.2688 - val_output_1_Top-5 Accuracy: 0.5250 - lr: 0.1000 - 72s/epoch - 92ms/step
Epoch 6/100
782/782 - 72s - loss: 3.0298 - output_1_loss: 2.8165 - output_2_loss: 0.0711 - output_1_Top-1 Accuracy: 0.3374 - output_1_Top-5 Accuracy: 0.6178 - val_loss: 3.1820 - val_output_1_loss: 2.9974 - val_output_2_loss: 0.0615 - val_output_1_Top-1 Accuracy: 0.3028 - val_output_1_Top-5 Accuracy: 0.5841 - lr: 0.1000 - 72s/epoch - 92ms/step
Epoch 7/100
782/782 - 72s - loss: 2.9522 - output_1_loss: 2.7367 - output_2_loss: 0.0718 - output_1_Top-1 Accuracy: 0.3532 - output_1_Top-5 Accuracy: 0.6340 - val_loss: 3.4460 - val_output_1_loss: 3.2593 - val_output_2_loss: 0.0622 - val_output_1_Top-1 Accuracy: 0.2628 - val_output_1_Top-5 Accuracy: 0.5311 - lr: 0.1000 - 72s/epoch - 93ms/step
Epoch 8/100
782/782 - 72s - loss: 2.8988 - output_1_loss: 2.6815 - output_2_loss: 0.0724 - output_1_Top-1 Accuracy: 0.3646 - output_1_Top-5 Accuracy: 0.6472 - val_loss: 3.1120 - val_output_1_loss: 2.9192 - val_output_2_loss: 0.0643 - val_output_1_Top-1 Accuracy: 0.3198 - val_output_1_Top-5 Accuracy: 0.6001 - lr: 0.1000 - 72s/epoch - 93ms/step
Epoch 9/100
782/782 - 72s - loss: 2.8440 - output_1_loss: 2.6244 - output_2_loss: 0.0732 - output_1_Top-1 Accuracy: 0.3757 - output_1_Top-5 Accuracy: 0.6553 - val_loss: 3.2431 - val_output_1_loss: 3.0500 - val_output_2_loss: 0.0644 - val_output_1_Top-1 Accuracy: 0.2994 - val_output_1_Top-5 Accuracy: 0.5677 - lr: 0.1000 - 72s/epoch - 93ms/step
Epoch 10/100
782/782 - 73s - loss: 2.7983 - output_1_loss: 2.5774 - output_2_loss: 0.0736 - output_1_Top-1 Accuracy: 0.3816 - output_1_Top-5 Accuracy: 0.6666 - val_loss: 3.1482 - val_output_1_loss: 2.9468 - val_output_2_loss: 0.0671 - val_output_1_Top-1 Accuracy: 0.3237 - val_output_1_Top-5 Accuracy: 0.5984 - lr: 0.1000 - 73s/epoch - 93ms/step
Epoch 11/100
782/782 - 72s - loss: 2.7489 - output_1_loss: 2.5259 - output_2_loss: 0.0743 - output_1_Top-1 Accuracy: 0.3934 - output_1_Top-5 Accuracy: 0.6759 - val_loss: 3.1046 - val_output_1_loss: 2.9092 - val_output_2_loss: 0.0651 - val_output_1_Top-1 Accuracy: 0.3275 - val_output_1_Top-5 Accuracy: 0.6018 - lr: 0.1000 - 72s/epoch - 92ms/step
Epoch 12/100
782/782 - 72s - loss: 2.7145 - output_1_loss: 2.4905 - output_2_loss: 0.0747 - output_1_Top-1 Accuracy: 0.4001 - output_1_Top-5 Accuracy: 0.6824 - val_loss: 3.0463 - val_output_1_loss: 2.8473 - val_output_2_loss: 0.0663 - val_output_1_Top-1 Accuracy: 0.3399 - val_output_1_Top-5 Accuracy: 0.6131 - lr: 0.1000 - 72s/epoch - 92ms/step
Epoch 13/100
782/782 - 72s - loss: 2.6818 - output_1_loss: 2.4563 - output_2_loss: 0.0752 - output_1_Top-1 Accuracy: 0.4062 - output_1_Top-5 Accuracy: 0.6892 - val_loss: 3.0145 - val_output_1_loss: 2.8203 - val_output_2_loss: 0.0647 - val_output_1_Top-1 Accuracy: 0.3459 - val_output_1_Top-5 Accuracy: 0.6157 - lr: 0.1000 - 72s/epoch - 92ms/step
Epoch 14/100
782/782 - 74s - loss: 2.6462 - output_1_loss: 2.4197 - output_2_loss: 0.0755 - output_1_Top-1 Accuracy: 0.4133 - output_1_Top-5 Accuracy: 0.6970 - val_loss: 3.2423 - val_output_1_loss: 3.0411 - val_output_2_loss: 0.0671 - val_output_1_Top-1 Accuracy: 0.3091 - val_output_1_Top-5 Accuracy: 0.5796 - lr: 0.1000 - 74s/epoch - 94ms/step
Epoch 15/100
782/782 - 74s - loss: 2.6155 - output_1_loss: 2.3878 - output_2_loss: 0.0759 - output_1_Top-1 Accuracy: 0.4199 - output_1_Top-5 Accuracy: 0.7038 - val_loss: 2.9300 - val_output_1_loss: 2.7261 - val_output_2_loss: 0.0680 - val_output_1_Top-1 Accuracy: 0.3588 - val_output_1_Top-5 Accuracy: 0.6373 - lr: 0.1000 - 74s/epoch - 95ms/step
Epoch 16/100
782/782 - 76s - loss: 2.5888 - output_1_loss: 2.3598 - output_2_loss: 0.0763 - output_1_Top-1 Accuracy: 0.4265 - output_1_Top-5 Accuracy: 0.7072 - val_loss: 2.9402 - val_output_1_loss: 2.7356 - val_output_2_loss: 0.0682 - val_output_1_Top-1 Accuracy: 0.3546 - val_output_1_Top-5 Accuracy: 0.6343 - lr: 0.1000 - 76s/epoch - 97ms/step
Epoch 17/100
782/782 - 74s - loss: 2.5614 - output_1_loss: 2.3314 - output_2_loss: 0.0767 - output_1_Top-1 Accuracy: 0.4320 - output_1_Top-5 Accuracy: 0.7137 - val_loss: 2.9458 - val_output_1_loss: 2.7450 - val_output_2_loss: 0.0669 - val_output_1_Top-1 Accuracy: 0.3562 - val_output_1_Top-5 Accuracy: 0.6296 - lr: 0.1000 - 74s/epoch - 94ms/step
Epoch 18/100

Epoch 18: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.
782/782 - 74s - loss: 2.5391 - output_1_loss: 2.3076 - output_2_loss: 0.0772 - output_1_Top-1 Accuracy: 0.4361 - output_1_Top-5 Accuracy: 0.7185 - val_loss: 3.0175 - val_output_1_loss: 2.8155 - val_output_2_loss: 0.0673 - val_output_1_Top-1 Accuracy: 0.3457 - val_output_1_Top-5 Accuracy: 0.6230 - lr: 0.1000 - 74s/epoch - 94ms/step
Epoch 19/100
782/782 - 101s - loss: 2.3240 - output_1_loss: 2.0850 - output_2_loss: 0.0797 - output_1_Top-1 Accuracy: 0.4824 - output_1_Top-5 Accuracy: 0.7565 - val_loss: 2.6350 - val_output_1_loss: 2.4160 - val_output_2_loss: 0.0730 - val_output_1_Top-1 Accuracy: 0.4176 - val_output_1_Top-5 Accuracy: 0.6938 - lr: 0.0100 - 101s/epoch - 130ms/step
Epoch 20/100
782/782 - 126s - loss: 2.2478 - output_1_loss: 2.0022 - output_2_loss: 0.0818 - output_1_Top-1 Accuracy: 0.4998 - output_1_Top-5 Accuracy: 0.7715 - val_loss: 2.6330 - val_output_1_loss: 2.4124 - val_output_2_loss: 0.0735 - val_output_1_Top-1 Accuracy: 0.4172 - val_output_1_Top-5 Accuracy: 0.6947 - lr: 0.0100 - 126s/epoch - 161ms/step
Epoch 21/100
782/782 - 94s - loss: 2.2221 - output_1_loss: 1.9758 - output_2_loss: 0.0821 - output_1_Top-1 Accuracy: 0.5076 - output_1_Top-5 Accuracy: 0.7742 - val_loss: 2.6308 - val_output_1_loss: 2.4075 - val_output_2_loss: 0.0744 - val_output_1_Top-1 Accuracy: 0.4253 - val_output_1_Top-5 Accuracy: 0.6951 - lr: 0.0100 - 94s/epoch - 120ms/step
Epoch 22/100
782/782 - 74s - loss: 2.2002 - output_1_loss: 1.9519 - output_2_loss: 0.0828 - output_1_Top-1 Accuracy: 0.5121 - output_1_Top-5 Accuracy: 0.7792 - val_loss: 2.6310 - val_output_1_loss: 2.4057 - val_output_2_loss: 0.0751 - val_output_1_Top-1 Accuracy: 0.4267 - val_output_1_Top-5 Accuracy: 0.6957 - lr: 0.0100 - 74s/epoch - 94ms/step
Epoch 23/100
782/782 - 74s - loss: 2.1848 - output_1_loss: 1.9357 - output_2_loss: 0.0831 - output_1_Top-1 Accuracy: 0.5145 - output_1_Top-5 Accuracy: 0.7831 - val_loss: 2.6348 - val_output_1_loss: 2.4097 - val_output_2_loss: 0.0750 - val_output_1_Top-1 Accuracy: 0.4201 - val_output_1_Top-5 Accuracy: 0.6950 - lr: 0.0100 - 74s/epoch - 95ms/step
Epoch 24/100

Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.
782/782 - 74s - loss: 2.1696 - output_1_loss: 1.9199 - output_2_loss: 0.0832 - output_1_Top-1 Accuracy: 0.5176 - output_1_Top-5 Accuracy: 0.7859 - val_loss: 2.6352 - val_output_1_loss: 2.4095 - val_output_2_loss: 0.0753 - val_output_1_Top-1 Accuracy: 0.4241 - val_output_1_Top-5 Accuracy: 0.6960 - lr: 0.0100 - 74s/epoch - 94ms/step
Epoch 25/100
782/782 - 77s - loss: 2.1394 - output_1_loss: 1.8893 - output_2_loss: 0.0834 - output_1_Top-1 Accuracy: 0.5254 - output_1_Top-5 Accuracy: 0.7909 - val_loss: 2.6244 - val_output_1_loss: 2.3973 - val_output_2_loss: 0.0757 - val_output_1_Top-1 Accuracy: 0.4281 - val_output_1_Top-5 Accuracy: 0.6989 - lr: 1.0000e-03 - 77s/epoch - 98ms/step
Epoch 26/100
782/782 - 74s - loss: 2.1283 - output_1_loss: 1.8771 - output_2_loss: 0.0837 - output_1_Top-1 Accuracy: 0.5277 - output_1_Top-5 Accuracy: 0.7934 - val_loss: 2.6252 - val_output_1_loss: 2.3971 - val_output_2_loss: 0.0760 - val_output_1_Top-1 Accuracy: 0.4277 - val_output_1_Top-5 Accuracy: 0.7003 - lr: 1.0000e-03 - 74s/epoch - 95ms/step
Epoch 27/100
782/782 - 73s - loss: 2.1251 - output_1_loss: 1.8735 - output_2_loss: 0.0839 - output_1_Top-1 Accuracy: 0.5295 - output_1_Top-5 Accuracy: 0.7935 - val_loss: 2.6263 - val_output_1_loss: 2.3979 - val_output_2_loss: 0.0761 - val_output_1_Top-1 Accuracy: 0.4258 - val_output_1_Top-5 Accuracy: 0.7002 - lr: 1.0000e-03 - 73s/epoch - 93ms/step
Epoch 28/100

Epoch 28: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.
782/782 - 74s - loss: 2.1263 - output_1_loss: 1.8743 - output_2_loss: 0.0840 - output_1_Top-1 Accuracy: 0.5287 - output_1_Top-5 Accuracy: 0.7930 - val_loss: 2.6271 - val_output_1_loss: 2.3986 - val_output_2_loss: 0.0762 - val_output_1_Top-1 Accuracy: 0.4260 - val_output_1_Top-5 Accuracy: 0.6998 - lr: 1.0000e-03 - 74s/epoch - 95ms/step
Epoch 29/100
782/782 - 74s - loss: 2.1235 - output_1_loss: 1.8712 - output_2_loss: 0.0841 - output_1_Top-1 Accuracy: 0.5293 - output_1_Top-5 Accuracy: 0.7937 - val_loss: 2.6271 - val_output_1_loss: 2.3984 - val_output_2_loss: 0.0762 - val_output_1_Top-1 Accuracy: 0.4269 - val_output_1_Top-5 Accuracy: 0.6992 - lr: 1.0000e-04 - 74s/epoch - 95ms/step
Epoch 30/100
782/782 - 77s - loss: 2.1214 - output_1_loss: 1.8696 - output_2_loss: 0.0839 - output_1_Top-1 Accuracy: 0.5313 - output_1_Top-5 Accuracy: 0.7943 - val_loss: 2.6278 - val_output_1_loss: 2.3990 - val_output_2_loss: 0.0763 - val_output_1_Top-1 Accuracy: 0.4267 - val_output_1_Top-5 Accuracy: 0.6998 - lr: 1.0000e-04 - 77s/epoch - 99ms/step
Epoch 31/100

Epoch 31: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.
782/782 - 74s - loss: 2.1264 - output_1_loss: 1.8742 - output_2_loss: 0.0841 - output_1_Top-1 Accuracy: 0.5279 - output_1_Top-5 Accuracy: 0.7932 - val_loss: 2.6274 - val_output_1_loss: 2.3986 - val_output_2_loss: 0.0763 - val_output_1_Top-1 Accuracy: 0.4259 - val_output_1_Top-5 Accuracy: 0.6999 - lr: 1.0000e-04 - 74s/epoch - 95ms/step
Epoch 32/100
782/782 - 74s - loss: 2.1218 - output_1_loss: 1.8696 - output_2_loss: 0.0841 - output_1_Top-1 Accuracy: 0.5281 - output_1_Top-5 Accuracy: 0.7955 - val_loss: 2.6271 - val_output_1_loss: 2.3985 - val_output_2_loss: 0.0762 - val_output_1_Top-1 Accuracy: 0.4269 - val_output_1_Top-5 Accuracy: 0.7003 - lr: 1.0000e-05 - 74s/epoch - 95ms/step
Epoch 33/100
782/782 - 104s - loss: 2.1169 - output_1_loss: 1.8646 - output_2_loss: 0.0841 - output_1_Top-1 Accuracy: 0.5310 - output_1_Top-5 Accuracy: 0.7958 - val_loss: 2.6280 - val_output_1_loss: 2.3991 - val_output_2_loss: 0.0763 - val_output_1_Top-1 Accuracy: 0.4258 - val_output_1_Top-5 Accuracy: 0.7003 - lr: 1.0000e-05 - 104s/epoch - 133ms/step
Epoch 34/100

Epoch 34: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.
782/782 - 103s - loss: 2.1180 - output_1_loss: 1.8659 - output_2_loss: 0.0840 - output_1_Top-1 Accuracy: 0.5288 - output_1_Top-5 Accuracy: 0.7952 - val_loss: 2.6271 - val_output_1_loss: 2.3982 - val_output_2_loss: 0.0763 - val_output_1_Top-1 Accuracy: 0.4264 - val_output_1_Top-5 Accuracy: 0.7010 - lr: 1.0000e-05 - 103s/epoch - 131ms/step
Epoch 35/100
782/782 - 74s - loss: 2.1202 - output_1_loss: 1.8682 - output_2_loss: 0.0840 - output_1_Top-1 Accuracy: 0.5278 - output_1_Top-5 Accuracy: 0.7956 - val_loss: 2.6281 - val_output_1_loss: 2.3991 - val_output_2_loss: 0.0763 - val_output_1_Top-1 Accuracy: 0.4255 - val_output_1_Top-5 Accuracy: 0.7002 - lr: 1.0000e-06 - 74s/epoch - 95ms/step
Epoch 35: early stopping
<keras.src.engine.input_layer.InputLayer object at 0x7f126e85faf0>
<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7f126c3b4ee0>
<keras.src.layers.normalization.batch_normalization.BatchNormalization object at 0x7f11d8ff3b20>
<keras.src.layers.core.activation.Activation object at 0x7f126c3fb0d0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d91463b0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8ddccd0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8b7efe0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8b7ada0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8b102e0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f126eda9cf0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f126edb05e0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f126edadb40>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f126edc3160>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11c25f4460>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11c25f4bb0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f126edb3070>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11c25b8ac0>
<keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D object at 0x7f11d8ced0c0>
<keras.src.layers.regularization.dropout.Dropout object at 0x7f11c25ae530>
<keras.src.layers.core.dense.Dense object at 0x7f11c25ae890>
<tempature_softmax_activation_layer.TempatureSoftmaxActivationLayer object at 0x7f11c25aed10>
<keras.src.engine.input_layer.InputLayer object at 0x7f126e85faf0>
<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7f126c3b4ee0>
<keras.src.layers.normalization.batch_normalization.BatchNormalization object at 0x7f11d8ff3b20>
<keras.src.layers.core.activation.Activation object at 0x7f126c3fb0d0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d91463b0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8ddccd0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8b7efe0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8b7ada0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8b102e0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f126eda9cf0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f126edb05e0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f126edadb40>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f126edc3160>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11c25f4460>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11c25f4bb0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f126edb3070>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11c25b8ac0>
<keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D object at 0x7f11d8ced0c0>
<keras.src.layers.regularization.dropout.Dropout object at 0x7f11c25ae530>
<keras.src.layers.core.dense.Dense object at 0x7f11c25ae890>
<tempature_softmax_activation_layer.TempatureSoftmaxActivationLayer object at 0x7f11c25aed10>
<keras.src.engine.input_layer.InputLayer object at 0x7f126e85faf0>
<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7f126c3b4ee0>
<keras.src.layers.normalization.batch_normalization.BatchNormalization object at 0x7f11d8ff3b20>
<keras.src.layers.core.activation.Activation object at 0x7f126c3fb0d0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d91463b0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8ddccd0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8b7efe0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8b7ada0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8b102e0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f126eda9cf0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f126edb05e0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f126edadb40>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f126edc3160>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11c25f4460>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11c25f4bb0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f126edb3070>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11c25b8ac0>
<keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D object at 0x7f11d8ced0c0>
<keras.src.layers.regularization.dropout.Dropout object at 0x7f11c25ae530>
<keras.src.layers.core.dense.Dense object at 0x7f11c25ae890>
<tempature_softmax_activation_layer.TempatureSoftmaxActivationLayer object at 0x7f11c25aed10>
<keras.src.engine.input_layer.InputLayer object at 0x7f126e85faf0>
<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7f126c3b4ee0>
<keras.src.layers.normalization.batch_normalization.BatchNormalization object at 0x7f11d8ff3b20>
<keras.src.layers.core.activation.Activation object at 0x7f126c3fb0d0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d91463b0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8ddccd0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8b7efe0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8b7ada0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8b102e0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f126eda9cf0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f126edb05e0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f126edadb40>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f126edc3160>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11c25f4460>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11c25f4bb0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f126edb3070>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11c25b8ac0>
<keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D object at 0x7f11d8ced0c0>
<keras.src.layers.regularization.dropout.Dropout object at 0x7f11c25ae530>
<keras.src.layers.core.dense.Dense object at 0x7f11c25ae890>
<tempature_softmax_activation_layer.TempatureSoftmaxActivationLayer object at 0x7f11c25aed10>
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 96, 96, 3)]       0         
                                                                 
 conv2d_14 (Conv2D)          (None, 47, 47, 24)        672       
                                                                 
 batch_normalization_40 (Ba  (None, 47, 47, 24)        96        
 tchNormalization)                                               
                                                                 
 activation_14 (Activation)  (None, 47, 47, 24)        0         
                                                                 
 linear_bottleneck_block_13  (None, 24, 24, 24)        1368      
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_14  (None, 24, 24, 24)        1368      
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_15  (None, 24, 24, 24)        1368      
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_16  (None, 24, 24, 24)        1368      
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_17  (None, 12, 12, 96)        4464      
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_18  (None, 12, 12, 96)        12384     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_19  (None, 12, 12, 96)        12384     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_20  (None, 12, 12, 96)        12384     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_21  (None, 6, 6, 168)         20664     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_22  (None, 6, 6, 168)         33768     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_23  (None, 6, 6, 168)         33768     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_24  (None, 3, 3, 192)         38256     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_25  (None, 3, 3, 384)         83712     
  (LinearBottleneckBlock)                                        
                                                                 
 global_average_pooling2d_1  (None, 384)               0         
  (GlobalAveragePooling2D)                                       
                                                                 
 dropout_1 (Dropout)         (None, 384)               0         
                                                                 
 output_1 (Dense)            (None, 200)               77000     
                                                                 
 output_2 (TempatureSoftmax  (None, 200)               0         
 ActivationLayer)                                                
                                                                 
=================================================================
Total params: 335024 (1.28 MB)
Trainable params: 326336 (1.24 MB)
Non-trainable params: 8688 (33.94 KB)
_________________________________________________________________
Epoch 1/100
782/782 - 96s - loss: 2.8646 - output_1_loss: 2.6454 - output_2_loss: 0.0731 - output_1_Top-1 Accuracy: 0.3699 - output_1_Top-5 Accuracy: 0.6555 - val_loss: 3.2509 - val_output_1_loss: 3.0613 - val_output_2_loss: 0.0632 - val_output_1_Top-1 Accuracy: 0.3036 - val_output_1_Top-5 Accuracy: 0.5716 - lr: 0.1000 - 96s/epoch - 123ms/step
Epoch 2/100
782/782 - 109s - loss: 2.7519 - output_1_loss: 2.5301 - output_2_loss: 0.0739 - output_1_Top-1 Accuracy: 0.3923 - output_1_Top-5 Accuracy: 0.6746 - val_loss: 3.2069 - val_output_1_loss: 3.0122 - val_output_2_loss: 0.0649 - val_output_1_Top-1 Accuracy: 0.3146 - val_output_1_Top-5 Accuracy: 0.5859 - lr: 0.1000 - 109s/epoch - 140ms/step
Epoch 3/100
782/782 - 113s - loss: 2.7019 - output_1_loss: 2.4779 - output_2_loss: 0.0747 - output_1_Top-1 Accuracy: 0.4035 - output_1_Top-5 Accuracy: 0.6846 - val_loss: 3.1230 - val_output_1_loss: 2.9284 - val_output_2_loss: 0.0649 - val_output_1_Top-1 Accuracy: 0.3280 - val_output_1_Top-5 Accuracy: 0.5962 - lr: 0.1000 - 113s/epoch - 145ms/step
Epoch 4/100
782/782 - 112s - loss: 2.6604 - output_1_loss: 2.4351 - output_2_loss: 0.0751 - output_1_Top-1 Accuracy: 0.4113 - output_1_Top-5 Accuracy: 0.6941 - val_loss: 2.9574 - val_output_1_loss: 2.7592 - val_output_2_loss: 0.0661 - val_output_1_Top-1 Accuracy: 0.3578 - val_output_1_Top-5 Accuracy: 0.6267 - lr: 0.1000 - 112s/epoch - 143ms/step
Epoch 5/100
782/782 - 113s - loss: 2.6281 - output_1_loss: 2.4015 - output_2_loss: 0.0755 - output_1_Top-1 Accuracy: 0.4197 - output_1_Top-5 Accuracy: 0.7002 - val_loss: 3.0762 - val_output_1_loss: 2.8807 - val_output_2_loss: 0.0652 - val_output_1_Top-1 Accuracy: 0.3360 - val_output_1_Top-5 Accuracy: 0.6075 - lr: 0.1000 - 113s/epoch - 144ms/step
Epoch 6/100
782/782 - 112s - loss: 2.5900 - output_1_loss: 2.3615 - output_2_loss: 0.0762 - output_1_Top-1 Accuracy: 0.4234 - output_1_Top-5 Accuracy: 0.7087 - val_loss: 3.0679 - val_output_1_loss: 2.8683 - val_output_2_loss: 0.0665 - val_output_1_Top-1 Accuracy: 0.3416 - val_output_1_Top-5 Accuracy: 0.6098 - lr: 0.1000 - 112s/epoch - 143ms/step
Epoch 7/100

Epoch 7: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.
782/782 - 112s - loss: 2.5710 - output_1_loss: 2.3419 - output_2_loss: 0.0764 - output_1_Top-1 Accuracy: 0.4301 - output_1_Top-5 Accuracy: 0.7107 - val_loss: 2.9837 - val_output_1_loss: 2.7798 - val_output_2_loss: 0.0680 - val_output_1_Top-1 Accuracy: 0.3487 - val_output_1_Top-5 Accuracy: 0.6285 - lr: 0.1000 - 112s/epoch - 143ms/step
Epoch 8/100
782/782 - 114s - loss: 2.3522 - output_1_loss: 2.1148 - output_2_loss: 0.0791 - output_1_Top-1 Accuracy: 0.4751 - output_1_Top-5 Accuracy: 0.7504 - val_loss: 2.6732 - val_output_1_loss: 2.4553 - val_output_2_loss: 0.0726 - val_output_1_Top-1 Accuracy: 0.4100 - val_output_1_Top-5 Accuracy: 0.6866 - lr: 0.0100 - 114s/epoch - 146ms/step
Epoch 9/100
782/782 - 112s - loss: 2.2751 - output_1_loss: 2.0323 - output_2_loss: 0.0809 - output_1_Top-1 Accuracy: 0.4940 - output_1_Top-5 Accuracy: 0.7640 - val_loss: 2.6707 - val_output_1_loss: 2.4517 - val_output_2_loss: 0.0730 - val_output_1_Top-1 Accuracy: 0.4139 - val_output_1_Top-5 Accuracy: 0.6878 - lr: 0.0100 - 112s/epoch - 143ms/step
Epoch 10/100
782/782 - 112s - loss: 2.2453 - output_1_loss: 2.0006 - output_2_loss: 0.0816 - output_1_Top-1 Accuracy: 0.5023 - output_1_Top-5 Accuracy: 0.7698 - val_loss: 2.6641 - val_output_1_loss: 2.4436 - val_output_2_loss: 0.0735 - val_output_1_Top-1 Accuracy: 0.4160 - val_output_1_Top-5 Accuracy: 0.6876 - lr: 0.0100 - 112s/epoch - 143ms/step
Epoch 11/100
782/782 - 112s - loss: 2.2257 - output_1_loss: 1.9803 - output_2_loss: 0.0818 - output_1_Top-1 Accuracy: 0.5048 - output_1_Top-5 Accuracy: 0.7745 - val_loss: 2.6730 - val_output_1_loss: 2.4504 - val_output_2_loss: 0.0742 - val_output_1_Top-1 Accuracy: 0.4153 - val_output_1_Top-5 Accuracy: 0.6899 - lr: 0.0100 - 112s/epoch - 143ms/step
Epoch 12/100
782/782 - 112s - loss: 2.2098 - output_1_loss: 1.9633 - output_2_loss: 0.0822 - output_1_Top-1 Accuracy: 0.5079 - output_1_Top-5 Accuracy: 0.7762 - val_loss: 2.6741 - val_output_1_loss: 2.4511 - val_output_2_loss: 0.0743 - val_output_1_Top-1 Accuracy: 0.4147 - val_output_1_Top-5 Accuracy: 0.6892 - lr: 0.0100 - 112s/epoch - 144ms/step
Epoch 13/100

Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.
782/782 - 113s - loss: 2.1906 - output_1_loss: 1.9427 - output_2_loss: 0.0826 - output_1_Top-1 Accuracy: 0.5115 - output_1_Top-5 Accuracy: 0.7804 - val_loss: 2.6770 - val_output_1_loss: 2.4523 - val_output_2_loss: 0.0749 - val_output_1_Top-1 Accuracy: 0.4150 - val_output_1_Top-5 Accuracy: 0.6900 - lr: 0.0100 - 113s/epoch - 144ms/step
Epoch 14/100
782/782 - 114s - loss: 2.1569 - output_1_loss: 1.9088 - output_2_loss: 0.0827 - output_1_Top-1 Accuracy: 0.5207 - output_1_Top-5 Accuracy: 0.7872 - val_loss: 2.6671 - val_output_1_loss: 2.4411 - val_output_2_loss: 0.0753 - val_output_1_Top-1 Accuracy: 0.4184 - val_output_1_Top-5 Accuracy: 0.6906 - lr: 1.0000e-03 - 114s/epoch - 146ms/step
Epoch 15/100
782/782 - 109s - loss: 2.1534 - output_1_loss: 1.9045 - output_2_loss: 0.0830 - output_1_Top-1 Accuracy: 0.5212 - output_1_Top-5 Accuracy: 0.7877 - val_loss: 2.6673 - val_output_1_loss: 2.4403 - val_output_2_loss: 0.0756 - val_output_1_Top-1 Accuracy: 0.4181 - val_output_1_Top-5 Accuracy: 0.6900 - lr: 1.0000e-03 - 109s/epoch - 140ms/step
Epoch 16/100

Epoch 16: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.
782/782 - 113s - loss: 2.1457 - output_1_loss: 1.8964 - output_2_loss: 0.0831 - output_1_Top-1 Accuracy: 0.5227 - output_1_Top-5 Accuracy: 0.7890 - val_loss: 2.6694 - val_output_1_loss: 2.4423 - val_output_2_loss: 0.0757 - val_output_1_Top-1 Accuracy: 0.4192 - val_output_1_Top-5 Accuracy: 0.6901 - lr: 1.0000e-03 - 113s/epoch - 145ms/step
Epoch 17/100
782/782 - 109s - loss: 2.1429 - output_1_loss: 1.8932 - output_2_loss: 0.0832 - output_1_Top-1 Accuracy: 0.5236 - output_1_Top-5 Accuracy: 0.7888 - val_loss: 2.6687 - val_output_1_loss: 2.4413 - val_output_2_loss: 0.0758 - val_output_1_Top-1 Accuracy: 0.4196 - val_output_1_Top-5 Accuracy: 0.6912 - lr: 1.0000e-04 - 109s/epoch - 140ms/step
Epoch 18/100
782/782 - 112s - loss: 2.1439 - output_1_loss: 1.8942 - output_2_loss: 0.0832 - output_1_Top-1 Accuracy: 0.5238 - output_1_Top-5 Accuracy: 0.7881 - val_loss: 2.6688 - val_output_1_loss: 2.4414 - val_output_2_loss: 0.0758 - val_output_1_Top-1 Accuracy: 0.4198 - val_output_1_Top-5 Accuracy: 0.6900 - lr: 1.0000e-04 - 112s/epoch - 143ms/step
Epoch 19/100

Epoch 19: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.
782/782 - 112s - loss: 2.1467 - output_1_loss: 1.8969 - output_2_loss: 0.0832 - output_1_Top-1 Accuracy: 0.5220 - output_1_Top-5 Accuracy: 0.7896 - val_loss: 2.6688 - val_output_1_loss: 2.4413 - val_output_2_loss: 0.0758 - val_output_1_Top-1 Accuracy: 0.4195 - val_output_1_Top-5 Accuracy: 0.6912 - lr: 1.0000e-04 - 112s/epoch - 143ms/step
Epoch 20/100
782/782 - 109s - loss: 2.1432 - output_1_loss: 1.8935 - output_2_loss: 0.0832 - output_1_Top-1 Accuracy: 0.5247 - output_1_Top-5 Accuracy: 0.7890 - val_loss: 2.6697 - val_output_1_loss: 2.4422 - val_output_2_loss: 0.0758 - val_output_1_Top-1 Accuracy: 0.4188 - val_output_1_Top-5 Accuracy: 0.6912 - lr: 1.0000e-05 - 109s/epoch - 140ms/step
Epoch 20: early stopping
<keras.src.engine.input_layer.InputLayer object at 0x7f126edc11e0>
<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7f11d242e620>
<keras.src.layers.normalization.batch_normalization.BatchNormalization object at 0x7f11d8a88b50>
<keras.src.layers.core.activation.Activation object at 0x7f126df07010>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d3661540>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d3660580>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d25a3b80>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d3687730>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d0782620>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8af56c0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cfe1beb0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cffa9cf0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d0761e10>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cfe3ca90>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cfe48370>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cffb2590>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cfe0cee0>
<keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D object at 0x7f11d8b784c0>
<keras.src.layers.regularization.dropout.Dropout object at 0x7f11cffb46a0>
<keras.src.layers.core.dense.Dense object at 0x7f11cffb58d0>
<tempature_softmax_activation_layer.TempatureSoftmaxActivationLayer object at 0x7f11d2259060>
<keras.src.engine.input_layer.InputLayer object at 0x7f126edc11e0>
<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7f11d242e620>
<keras.src.layers.normalization.batch_normalization.BatchNormalization object at 0x7f11d8a88b50>
<keras.src.layers.core.activation.Activation object at 0x7f126df07010>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d3661540>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d3660580>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d25a3b80>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d3687730>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d0782620>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8af56c0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cfe1beb0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cffa9cf0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d0761e10>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cfe3ca90>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cfe48370>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cffb2590>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cfe0cee0>
<keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D object at 0x7f11d8b784c0>
<keras.src.layers.regularization.dropout.Dropout object at 0x7f11cffb46a0>
<keras.src.layers.core.dense.Dense object at 0x7f11cffb58d0>
<tempature_softmax_activation_layer.TempatureSoftmaxActivationLayer object at 0x7f11d2259060>
<keras.src.engine.input_layer.InputLayer object at 0x7f126edc11e0>
<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7f11d242e620>
<keras.src.layers.normalization.batch_normalization.BatchNormalization object at 0x7f11d8a88b50>
<keras.src.layers.core.activation.Activation object at 0x7f126df07010>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d3661540>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d3660580>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d25a3b80>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d3687730>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d0782620>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8af56c0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cfe1beb0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cffa9cf0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d0761e10>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cfe3ca90>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cfe48370>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cffb2590>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cfe0cee0>
<keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D object at 0x7f11d8b784c0>
<keras.src.layers.regularization.dropout.Dropout object at 0x7f11cffb46a0>
<keras.src.layers.core.dense.Dense object at 0x7f11cffb58d0>
<tempature_softmax_activation_layer.TempatureSoftmaxActivationLayer object at 0x7f11d2259060>
<keras.src.engine.input_layer.InputLayer object at 0x7f126edc11e0>
<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7f11d242e620>
<keras.src.layers.normalization.batch_normalization.BatchNormalization object at 0x7f11d8a88b50>
<keras.src.layers.core.activation.Activation object at 0x7f126df07010>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d3661540>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d3660580>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d25a3b80>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d3687730>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d0782620>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8af56c0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cfe1beb0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cffa9cf0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d0761e10>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cfe3ca90>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cfe48370>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cffb2590>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cfe0cee0>
<keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D object at 0x7f11d8b784c0>
<keras.src.layers.regularization.dropout.Dropout object at 0x7f11cffb46a0>
<keras.src.layers.core.dense.Dense object at 0x7f11cffb58d0>
<tempature_softmax_activation_layer.TempatureSoftmaxActivationLayer object at 0x7f11d2259060>
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 64, 64, 3)]       0         
                                                                 
 conv2d_28 (Conv2D)          (None, 31, 31, 24)        672       
                                                                 
 batch_normalization_80 (Ba  (None, 31, 31, 24)        96        
 tchNormalization)                                               
                                                                 
 activation_28 (Activation)  (None, 31, 31, 24)        0         
                                                                 
 linear_bottleneck_block_26  (None, 16, 16, 24)        1368      
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_27  (None, 16, 16, 24)        1368      
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_28  (None, 16, 16, 24)        1368      
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_29  (None, 16, 16, 24)        1368      
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_30  (None, 8, 8, 96)          4464      
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_31  (None, 8, 8, 96)          12384     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_32  (None, 8, 8, 96)          12384     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_33  (None, 8, 8, 96)          12384     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_34  (None, 4, 4, 168)         20664     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_35  (None, 4, 4, 168)         33768     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_36  (None, 4, 4, 168)         33768     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_37  (None, 2, 2, 192)         38256     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_38  (None, 2, 2, 384)         83712     
  (LinearBottleneckBlock)                                        
                                                                 
 global_average_pooling2d_2  (None, 384)               0         
  (GlobalAveragePooling2D)                                       
                                                                 
 dropout_2 (Dropout)         (None, 384)               0         
                                                                 
 output_1 (Dense)            (None, 200)               77000     
                                                                 
 output_2 (TempatureSoftmax  (None, 200)               0         
 ActivationLayer)                                                
                                                                 
=================================================================
Total params: 335024 (1.28 MB)
Trainable params: 326336 (1.24 MB)
Non-trainable params: 8688 (33.94 KB)
_________________________________________________________________
Epoch 1/100
782/782 - 116s - loss: 3.3467 - output_1_loss: 3.1438 - output_2_loss: 0.0676 - output_1_Top-1 Accuracy: 0.2821 - output_1_Top-5 Accuracy: 0.5546 - val_loss: 3.3873 - val_output_1_loss: 3.2087 - val_output_2_loss: 0.0595 - val_output_1_Top-1 Accuracy: 0.2776 - val_output_1_Top-5 Accuracy: 0.5355 - lr: 0.1000 - 116s/epoch - 148ms/step
Epoch 2/100
782/782 - 88s - loss: 3.1450 - output_1_loss: 2.9383 - output_2_loss: 0.0689 - output_1_Top-1 Accuracy: 0.3178 - output_1_Top-5 Accuracy: 0.5957 - val_loss: 3.2703 - val_output_1_loss: 3.0898 - val_output_2_loss: 0.0602 - val_output_1_Top-1 Accuracy: 0.2940 - val_output_1_Top-5 Accuracy: 0.5627 - lr: 0.1000 - 88s/epoch - 112ms/step
Epoch 3/100
782/782 - 87s - loss: 3.0606 - output_1_loss: 2.8513 - output_2_loss: 0.0698 - output_1_Top-1 Accuracy: 0.3317 - output_1_Top-5 Accuracy: 0.6112 - val_loss: 3.4638 - val_output_1_loss: 3.2835 - val_output_2_loss: 0.0601 - val_output_1_Top-1 Accuracy: 0.2630 - val_output_1_Top-5 Accuracy: 0.5234 - lr: 0.1000 - 87s/epoch - 112ms/step
Epoch 4/100
782/782 - 88s - loss: 2.9960 - output_1_loss: 2.7845 - output_2_loss: 0.0705 - output_1_Top-1 Accuracy: 0.3450 - output_1_Top-5 Accuracy: 0.6263 - val_loss: 3.3541 - val_output_1_loss: 3.1718 - val_output_2_loss: 0.0608 - val_output_1_Top-1 Accuracy: 0.2760 - val_output_1_Top-5 Accuracy: 0.5455 - lr: 0.1000 - 88s/epoch - 112ms/step
Epoch 5/100

Epoch 5: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.
782/782 - 84s - loss: 2.9415 - output_1_loss: 2.7282 - output_2_loss: 0.0711 - output_1_Top-1 Accuracy: 0.3539 - output_1_Top-5 Accuracy: 0.6356 - val_loss: 3.3362 - val_output_1_loss: 3.1438 - val_output_2_loss: 0.0641 - val_output_1_Top-1 Accuracy: 0.2880 - val_output_1_Top-5 Accuracy: 0.5551 - lr: 0.1000 - 84s/epoch - 108ms/step
Epoch 6/100
782/782 - 87s - loss: 2.7241 - output_1_loss: 2.5046 - output_2_loss: 0.0732 - output_1_Top-1 Accuracy: 0.3972 - output_1_Top-5 Accuracy: 0.6782 - val_loss: 2.9028 - val_output_1_loss: 2.7054 - val_output_2_loss: 0.0658 - val_output_1_Top-1 Accuracy: 0.3625 - val_output_1_Top-5 Accuracy: 0.6375 - lr: 0.0100 - 87s/epoch - 112ms/step
Epoch 7/100
782/782 - 88s - loss: 2.6415 - output_1_loss: 2.4180 - output_2_loss: 0.0745 - output_1_Top-1 Accuracy: 0.4161 - output_1_Top-5 Accuracy: 0.6964 - val_loss: 2.8786 - val_output_1_loss: 2.6801 - val_output_2_loss: 0.0662 - val_output_1_Top-1 Accuracy: 0.3663 - val_output_1_Top-5 Accuracy: 0.6409 - lr: 0.0100 - 88s/epoch - 113ms/step
Epoch 8/100
782/782 - 87s - loss: 2.6175 - output_1_loss: 2.3929 - output_2_loss: 0.0749 - output_1_Top-1 Accuracy: 0.4199 - output_1_Top-5 Accuracy: 0.6999 - val_loss: 2.8833 - val_output_1_loss: 2.6830 - val_output_2_loss: 0.0668 - val_output_1_Top-1 Accuracy: 0.3668 - val_output_1_Top-5 Accuracy: 0.6417 - lr: 0.0100 - 87s/epoch - 112ms/step
Epoch 9/100
782/782 - 88s - loss: 2.5989 - output_1_loss: 2.3733 - output_2_loss: 0.0752 - output_1_Top-1 Accuracy: 0.4245 - output_1_Top-5 Accuracy: 0.7024 - val_loss: 2.8739 - val_output_1_loss: 2.6729 - val_output_2_loss: 0.0670 - val_output_1_Top-1 Accuracy: 0.3667 - val_output_1_Top-5 Accuracy: 0.6446 - lr: 0.0100 - 88s/epoch - 112ms/step
Epoch 10/100
782/782 - 88s - loss: 2.5835 - output_1_loss: 2.3577 - output_2_loss: 0.0753 - output_1_Top-1 Accuracy: 0.4282 - output_1_Top-5 Accuracy: 0.7058 - val_loss: 2.8737 - val_output_1_loss: 2.6716 - val_output_2_loss: 0.0674 - val_output_1_Top-1 Accuracy: 0.3712 - val_output_1_Top-5 Accuracy: 0.6454 - lr: 0.0100 - 88s/epoch - 112ms/step
Epoch 11/100
782/782 - 88s - loss: 2.5625 - output_1_loss: 2.3354 - output_2_loss: 0.0757 - output_1_Top-1 Accuracy: 0.4320 - output_1_Top-5 Accuracy: 0.7103 - val_loss: 2.8800 - val_output_1_loss: 2.6780 - val_output_2_loss: 0.0673 - val_output_1_Top-1 Accuracy: 0.3699 - val_output_1_Top-5 Accuracy: 0.6455 - lr: 0.0100 - 88s/epoch - 112ms/step
Epoch 12/100
782/782 - 87s - loss: 2.5496 - output_1_loss: 2.3218 - output_2_loss: 0.0760 - output_1_Top-1 Accuracy: 0.4333 - output_1_Top-5 Accuracy: 0.7116 - val_loss: 2.8767 - val_output_1_loss: 2.6734 - val_output_2_loss: 0.0678 - val_output_1_Top-1 Accuracy: 0.3712 - val_output_1_Top-5 Accuracy: 0.6476 - lr: 0.0100 - 87s/epoch - 112ms/step
Epoch 13/100

Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.
782/782 - 87s - loss: 2.5396 - output_1_loss: 2.3116 - output_2_loss: 0.0760 - output_1_Top-1 Accuracy: 0.4363 - output_1_Top-5 Accuracy: 0.7155 - val_loss: 2.8814 - val_output_1_loss: 2.6777 - val_output_2_loss: 0.0679 - val_output_1_Top-1 Accuracy: 0.3700 - val_output_1_Top-5 Accuracy: 0.6491 - lr: 0.0100 - 87s/epoch - 112ms/step
Epoch 14/100
782/782 - 88s - loss: 2.5062 - output_1_loss: 2.2772 - output_2_loss: 0.0763 - output_1_Top-1 Accuracy: 0.4424 - output_1_Top-5 Accuracy: 0.7205 - val_loss: 2.8599 - val_output_1_loss: 2.6545 - val_output_2_loss: 0.0685 - val_output_1_Top-1 Accuracy: 0.3736 - val_output_1_Top-5 Accuracy: 0.6532 - lr: 1.0000e-03 - 88s/epoch - 112ms/step
Epoch 15/100
782/782 - 38s - loss: 2.4963 - output_1_loss: 2.2670 - output_2_loss: 0.0764 - output_1_Top-1 Accuracy: 0.4454 - output_1_Top-5 Accuracy: 0.7230 - val_loss: 2.8592 - val_output_1_loss: 2.6536 - val_output_2_loss: 0.0685 - val_output_1_Top-1 Accuracy: 0.3737 - val_output_1_Top-5 Accuracy: 0.6532 - lr: 1.0000e-03 - 38s/epoch - 49ms/step
Epoch 16/100
782/782 - 87s - loss: 2.4933 - output_1_loss: 2.2636 - output_2_loss: 0.0766 - output_1_Top-1 Accuracy: 0.4472 - output_1_Top-5 Accuracy: 0.7240 - val_loss: 2.8598 - val_output_1_loss: 2.6539 - val_output_2_loss: 0.0687 - val_output_1_Top-1 Accuracy: 0.3738 - val_output_1_Top-5 Accuracy: 0.6521 - lr: 1.0000e-03 - 87s/epoch - 112ms/step
Epoch 17/100
782/782 - 85s - loss: 2.4907 - output_1_loss: 2.2608 - output_2_loss: 0.0766 - output_1_Top-1 Accuracy: 0.4460 - output_1_Top-5 Accuracy: 0.7234 - val_loss: 2.8602 - val_output_1_loss: 2.6539 - val_output_2_loss: 0.0688 - val_output_1_Top-1 Accuracy: 0.3733 - val_output_1_Top-5 Accuracy: 0.6530 - lr: 1.0000e-03 - 85s/epoch - 108ms/step
Epoch 18/100

Epoch 18: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.
782/782 - 86s - loss: 2.4890 - output_1_loss: 2.2589 - output_2_loss: 0.0767 - output_1_Top-1 Accuracy: 0.4467 - output_1_Top-5 Accuracy: 0.7258 - val_loss: 2.8610 - val_output_1_loss: 2.6545 - val_output_2_loss: 0.0688 - val_output_1_Top-1 Accuracy: 0.3740 - val_output_1_Top-5 Accuracy: 0.6524 - lr: 1.0000e-03 - 86s/epoch - 110ms/step
Epoch 19/100
782/782 - 86s - loss: 2.4881 - output_1_loss: 2.2576 - output_2_loss: 0.0768 - output_1_Top-1 Accuracy: 0.4460 - output_1_Top-5 Accuracy: 0.7239 - val_loss: 2.8598 - val_output_1_loss: 2.6532 - val_output_2_loss: 0.0689 - val_output_1_Top-1 Accuracy: 0.3737 - val_output_1_Top-5 Accuracy: 0.6532 - lr: 1.0000e-04 - 86s/epoch - 110ms/step
Epoch 20/100
782/782 - 84s - loss: 2.4849 - output_1_loss: 2.2546 - output_2_loss: 0.0768 - output_1_Top-1 Accuracy: 0.4474 - output_1_Top-5 Accuracy: 0.7252 - val_loss: 2.8609 - val_output_1_loss: 2.6540 - val_output_2_loss: 0.0689 - val_output_1_Top-1 Accuracy: 0.3729 - val_output_1_Top-5 Accuracy: 0.6537 - lr: 1.0000e-04 - 84s/epoch - 108ms/step
Epoch 21/100

Epoch 21: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.
782/782 - 85s - loss: 2.4835 - output_1_loss: 2.2534 - output_2_loss: 0.0767 - output_1_Top-1 Accuracy: 0.4483 - output_1_Top-5 Accuracy: 0.7250 - val_loss: 2.8600 - val_output_1_loss: 2.6536 - val_output_2_loss: 0.0688 - val_output_1_Top-1 Accuracy: 0.3722 - val_output_1_Top-5 Accuracy: 0.6534 - lr: 1.0000e-04 - 85s/epoch - 109ms/step
Epoch 22/100
782/782 - 88s - loss: 2.4828 - output_1_loss: 2.2525 - output_2_loss: 0.0768 - output_1_Top-1 Accuracy: 0.4483 - output_1_Top-5 Accuracy: 0.7260 - val_loss: 2.8603 - val_output_1_loss: 2.6535 - val_output_2_loss: 0.0689 - val_output_1_Top-1 Accuracy: 0.3724 - val_output_1_Top-5 Accuracy: 0.6534 - lr: 1.0000e-05 - 88s/epoch - 112ms/step
Epoch 23/100
782/782 - 86s - loss: 2.4809 - output_1_loss: 2.2505 - output_2_loss: 0.0768 - output_1_Top-1 Accuracy: 0.4478 - output_1_Top-5 Accuracy: 0.7266 - val_loss: 2.8601 - val_output_1_loss: 2.6533 - val_output_2_loss: 0.0689 - val_output_1_Top-1 Accuracy: 0.3733 - val_output_1_Top-5 Accuracy: 0.6534 - lr: 1.0000e-05 - 86s/epoch - 110ms/step
Epoch 24/100

Epoch 24: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.
782/782 - 88s - loss: 2.4856 - output_1_loss: 2.2551 - output_2_loss: 0.0768 - output_1_Top-1 Accuracy: 0.4476 - output_1_Top-5 Accuracy: 0.7244 - val_loss: 2.8601 - val_output_1_loss: 2.6535 - val_output_2_loss: 0.0689 - val_output_1_Top-1 Accuracy: 0.3735 - val_output_1_Top-5 Accuracy: 0.6533 - lr: 1.0000e-05 - 88s/epoch - 112ms/step
Epoch 25/100
782/782 - 87s - loss: 2.4810 - output_1_loss: 2.2508 - output_2_loss: 0.0767 - output_1_Top-1 Accuracy: 0.4469 - output_1_Top-5 Accuracy: 0.7263 - val_loss: 2.8607 - val_output_1_loss: 2.6540 - val_output_2_loss: 0.0689 - val_output_1_Top-1 Accuracy: 0.3729 - val_output_1_Top-5 Accuracy: 0.6531 - lr: 1.0000e-06 - 87s/epoch - 112ms/step
Epoch 25: early stopping
<keras.src.engine.input_layer.InputLayer object at 0x7f11d93b77c0>
<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7f11d99a1c60>
<keras.src.layers.normalization.batch_normalization.BatchNormalization object at 0x7f11d99a07f0>
<keras.src.layers.core.activation.Activation object at 0x7f11d992d000>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d992ce80>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c1ec0d0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c207220>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cffd72e0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d07631c0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c2729e0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c2615d0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c260a90>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d9948460>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c21aec0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d995bb50>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c19e410>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d9955120>
<keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D object at 0x7f11cfe3ee00>
<keras.src.layers.regularization.dropout.Dropout object at 0x7f120c237cd0>
<keras.src.layers.core.dense.Dense object at 0x7f120c234a30>
<tempature_softmax_activation_layer.TempatureSoftmaxActivationLayer object at 0x7f120c234b50>
<keras.src.engine.input_layer.InputLayer object at 0x7f11d93b77c0>
<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7f11d99a1c60>
<keras.src.layers.normalization.batch_normalization.BatchNormalization object at 0x7f11d99a07f0>
<keras.src.layers.core.activation.Activation object at 0x7f11d992d000>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d992ce80>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c1ec0d0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c207220>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cffd72e0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d07631c0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c2729e0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c2615d0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c260a90>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d9948460>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c21aec0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d995bb50>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c19e410>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d9955120>
<keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D object at 0x7f11cfe3ee00>
<keras.src.layers.regularization.dropout.Dropout object at 0x7f120c237cd0>
<keras.src.layers.core.dense.Dense object at 0x7f120c234a30>
<tempature_softmax_activation_layer.TempatureSoftmaxActivationLayer object at 0x7f120c234b50>
<keras.src.engine.input_layer.InputLayer object at 0x7f11d93b77c0>
<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7f11d99a1c60>
<keras.src.layers.normalization.batch_normalization.BatchNormalization object at 0x7f11d99a07f0>
<keras.src.layers.core.activation.Activation object at 0x7f11d992d000>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d992ce80>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c1ec0d0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c207220>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cffd72e0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d07631c0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c2729e0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c2615d0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c260a90>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d9948460>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c21aec0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d995bb50>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c19e410>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d9955120>
<keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D object at 0x7f11cfe3ee00>
<keras.src.layers.regularization.dropout.Dropout object at 0x7f120c237cd0>
<keras.src.layers.core.dense.Dense object at 0x7f120c234a30>
<tempature_softmax_activation_layer.TempatureSoftmaxActivationLayer object at 0x7f120c234b50>
<keras.src.engine.input_layer.InputLayer object at 0x7f11d93b77c0>
<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7f11d99a1c60>
<keras.src.layers.normalization.batch_normalization.BatchNormalization object at 0x7f11d99a07f0>
<keras.src.layers.core.activation.Activation object at 0x7f11d992d000>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d992ce80>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c1ec0d0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c207220>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11cffd72e0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d07631c0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c2729e0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c2615d0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c260a90>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d9948460>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c21aec0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d995bb50>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f120c19e410>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d9955120>
<keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D object at 0x7f11cfe3ee00>
<keras.src.layers.regularization.dropout.Dropout object at 0x7f120c237cd0>
<keras.src.layers.core.dense.Dense object at 0x7f120c234a30>
<tempature_softmax_activation_layer.TempatureSoftmaxActivationLayer object at 0x7f120c234b50>
Model: "model_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 48, 48, 3)]       0         
                                                                 
 conv2d_42 (Conv2D)          (None, 23, 23, 24)        672       
                                                                 
 batch_normalization_120 (B  (None, 23, 23, 24)        96        
 atchNormalization)                                              
                                                                 
 activation_42 (Activation)  (None, 23, 23, 24)        0         
                                                                 
 linear_bottleneck_block_39  (None, 12, 12, 24)        1368      
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_40  (None, 12, 12, 24)        1368      
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_41  (None, 12, 12, 24)        1368      
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_42  (None, 12, 12, 24)        1368      
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_43  (None, 6, 6, 96)          4464      
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_44  (None, 6, 6, 96)          12384     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_45  (None, 6, 6, 96)          12384     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_46  (None, 6, 6, 96)          12384     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_47  (None, 3, 3, 168)         20664     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_48  (None, 3, 3, 168)         33768     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_49  (None, 3, 3, 168)         33768     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_50  (None, 2, 2, 192)         38256     
  (LinearBottleneckBlock)                                        
                                                                 
 linear_bottleneck_block_51  (None, 2, 2, 384)         83712     
  (LinearBottleneckBlock)                                        
                                                                 
 global_average_pooling2d_3  (None, 384)               0         
  (GlobalAveragePooling2D)                                       
                                                                 
 dropout_3 (Dropout)         (None, 384)               0         
                                                                 
 output_1 (Dense)            (None, 200)               77000     
                                                                 
 output_2 (TempatureSoftmax  (None, 200)               0         
 ActivationLayer)                                                
                                                                 
=================================================================
Total params: 335024 (1.28 MB)
Trainable params: 326336 (1.24 MB)
Non-trainable params: 8688 (33.94 KB)
_________________________________________________________________
Epoch 1/100
782/782 - 55s - loss: 3.3733 - output_1_loss: 3.1721 - output_2_loss: 0.0671 - output_1_Top-1 Accuracy: 0.2761 - output_1_Top-5 Accuracy: 0.5446 - val_loss: 3.6282 - val_output_1_loss: 3.4504 - val_output_2_loss: 0.0593 - val_output_1_Top-1 Accuracy: 0.2386 - val_output_1_Top-5 Accuracy: 0.4874 - lr: 0.1000 - 55s/epoch - 71ms/step
Epoch 2/100
782/782 - 65s - loss: 3.2402 - output_1_loss: 3.0362 - output_2_loss: 0.0680 - output_1_Top-1 Accuracy: 0.2985 - output_1_Top-5 Accuracy: 0.5725 - val_loss: 3.4440 - val_output_1_loss: 3.2649 - val_output_2_loss: 0.0597 - val_output_1_Top-1 Accuracy: 0.2616 - val_output_1_Top-5 Accuracy: 0.5249 - lr: 0.1000 - 65s/epoch - 83ms/step
Epoch 3/100
782/782 - 70s - loss: 3.1659 - output_1_loss: 2.9598 - output_2_loss: 0.0687 - output_1_Top-1 Accuracy: 0.3122 - output_1_Top-5 Accuracy: 0.5885 - val_loss: 3.3358 - val_output_1_loss: 3.1561 - val_output_2_loss: 0.0599 - val_output_1_Top-1 Accuracy: 0.2772 - val_output_1_Top-5 Accuracy: 0.5450 - lr: 0.1000 - 70s/epoch - 89ms/step
Epoch 4/100
782/782 - 70s - loss: 3.1119 - output_1_loss: 2.9043 - output_2_loss: 0.0692 - output_1_Top-1 Accuracy: 0.3211 - output_1_Top-5 Accuracy: 0.5996 - val_loss: 3.5809 - val_output_1_loss: 3.3982 - val_output_2_loss: 0.0609 - val_output_1_Top-1 Accuracy: 0.2475 - val_output_1_Top-5 Accuracy: 0.5097 - lr: 0.1000 - 70s/epoch - 89ms/step
Epoch 5/100
782/782 - 66s - loss: 3.0704 - output_1_loss: 2.8615 - output_2_loss: 0.0696 - output_1_Top-1 Accuracy: 0.3290 - output_1_Top-5 Accuracy: 0.6077 - val_loss: 3.4527 - val_output_1_loss: 3.2693 - val_output_2_loss: 0.0611 - val_output_1_Top-1 Accuracy: 0.2713 - val_output_1_Top-5 Accuracy: 0.5221 - lr: 0.1000 - 66s/epoch - 84ms/step
Epoch 6/100

Epoch 6: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.
782/782 - 69s - loss: 3.0274 - output_1_loss: 2.8173 - output_2_loss: 0.0700 - output_1_Top-1 Accuracy: 0.3376 - output_1_Top-5 Accuracy: 0.6180 - val_loss: 3.4431 - val_output_1_loss: 3.2590 - val_output_2_loss: 0.0614 - val_output_1_Top-1 Accuracy: 0.2690 - val_output_1_Top-5 Accuracy: 0.5318 - lr: 0.1000 - 69s/epoch - 88ms/step
Epoch 7/100
782/782 - 69s - loss: 2.8143 - output_1_loss: 2.5986 - output_2_loss: 0.0719 - output_1_Top-1 Accuracy: 0.3783 - output_1_Top-5 Accuracy: 0.6594 - val_loss: 2.9850 - val_output_1_loss: 2.7915 - val_output_2_loss: 0.0645 - val_output_1_Top-1 Accuracy: 0.3493 - val_output_1_Top-5 Accuracy: 0.6207 - lr: 0.0100 - 69s/epoch - 88ms/step
Epoch 8/100
782/782 - 69s - loss: 2.7384 - output_1_loss: 2.5197 - output_2_loss: 0.0729 - output_1_Top-1 Accuracy: 0.3947 - output_1_Top-5 Accuracy: 0.6754 - val_loss: 2.9705 - val_output_1_loss: 2.7758 - val_output_2_loss: 0.0649 - val_output_1_Top-1 Accuracy: 0.3502 - val_output_1_Top-5 Accuracy: 0.6240 - lr: 0.0100 - 69s/epoch - 88ms/step
Epoch 9/100
782/782 - 69s - loss: 2.7057 - output_1_loss: 2.4855 - output_2_loss: 0.0734 - output_1_Top-1 Accuracy: 0.3995 - output_1_Top-5 Accuracy: 0.6807 - val_loss: 2.9599 - val_output_1_loss: 2.7641 - val_output_2_loss: 0.0653 - val_output_1_Top-1 Accuracy: 0.3526 - val_output_1_Top-5 Accuracy: 0.6300 - lr: 0.0100 - 69s/epoch - 88ms/step
Epoch 10/100
782/782 - 69s - loss: 2.6855 - output_1_loss: 2.4645 - output_2_loss: 0.0737 - output_1_Top-1 Accuracy: 0.4062 - output_1_Top-5 Accuracy: 0.6848 - val_loss: 2.9832 - val_output_1_loss: 2.7853 - val_output_2_loss: 0.0660 - val_output_1_Top-1 Accuracy: 0.3492 - val_output_1_Top-5 Accuracy: 0.6244 - lr: 0.0100 - 69s/epoch - 88ms/step
Epoch 11/100
782/782 - 70s - loss: 2.6703 - output_1_loss: 2.4481 - output_2_loss: 0.0740 - output_1_Top-1 Accuracy: 0.4085 - output_1_Top-5 Accuracy: 0.6883 - val_loss: 2.9637 - val_output_1_loss: 2.7671 - val_output_2_loss: 0.0655 - val_output_1_Top-1 Accuracy: 0.3538 - val_output_1_Top-5 Accuracy: 0.6258 - lr: 0.0100 - 70s/epoch - 90ms/step
Epoch 12/100

Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.
782/782 - 70s - loss: 2.6598 - output_1_loss: 2.4374 - output_2_loss: 0.0741 - output_1_Top-1 Accuracy: 0.4101 - output_1_Top-5 Accuracy: 0.6903 - val_loss: 2.9657 - val_output_1_loss: 2.7687 - val_output_2_loss: 0.0657 - val_output_1_Top-1 Accuracy: 0.3522 - val_output_1_Top-5 Accuracy: 0.6263 - lr: 0.0100 - 70s/epoch - 90ms/step
Epoch 13/100
782/782 - 70s - loss: 2.6206 - output_1_loss: 2.3978 - output_2_loss: 0.0742 - output_1_Top-1 Accuracy: 0.4186 - output_1_Top-5 Accuracy: 0.6969 - val_loss: 2.9493 - val_output_1_loss: 2.7505 - val_output_2_loss: 0.0663 - val_output_1_Top-1 Accuracy: 0.3581 - val_output_1_Top-5 Accuracy: 0.6294 - lr: 1.0000e-03 - 70s/epoch - 89ms/step
Epoch 14/100
782/782 - 66s - loss: 2.6127 - output_1_loss: 2.3895 - output_2_loss: 0.0744 - output_1_Top-1 Accuracy: 0.4201 - output_1_Top-5 Accuracy: 0.7008 - val_loss: 2.9487 - val_output_1_loss: 2.7492 - val_output_2_loss: 0.0665 - val_output_1_Top-1 Accuracy: 0.3568 - val_output_1_Top-5 Accuracy: 0.6305 - lr: 1.0000e-03 - 66s/epoch - 84ms/step
Epoch 15/100
782/782 - 69s - loss: 2.6085 - output_1_loss: 2.3848 - output_2_loss: 0.0746 - output_1_Top-1 Accuracy: 0.4193 - output_1_Top-5 Accuracy: 0.7002 - val_loss: 2.9485 - val_output_1_loss: 2.7486 - val_output_2_loss: 0.0666 - val_output_1_Top-1 Accuracy: 0.3549 - val_output_1_Top-5 Accuracy: 0.6297 - lr: 1.0000e-03 - 69s/epoch - 88ms/step
Epoch 16/100
782/782 - 69s - loss: 2.6074 - output_1_loss: 2.3832 - output_2_loss: 0.0747 - output_1_Top-1 Accuracy: 0.4207 - output_1_Top-5 Accuracy: 0.6995 - val_loss: 2.9487 - val_output_1_loss: 2.7485 - val_output_2_loss: 0.0667 - val_output_1_Top-1 Accuracy: 0.3571 - val_output_1_Top-5 Accuracy: 0.6311 - lr: 1.0000e-03 - 69s/epoch - 88ms/step
Epoch 17/100
782/782 - 66s - loss: 2.6082 - output_1_loss: 2.3839 - output_2_loss: 0.0748 - output_1_Top-1 Accuracy: 0.4215 - output_1_Top-5 Accuracy: 0.6991 - val_loss: 2.9468 - val_output_1_loss: 2.7467 - val_output_2_loss: 0.0667 - val_output_1_Top-1 Accuracy: 0.3569 - val_output_1_Top-5 Accuracy: 0.6306 - lr: 1.0000e-03 - 66s/epoch - 85ms/step
Epoch 18/100
782/782 - 67s - loss: 2.6037 - output_1_loss: 2.3793 - output_2_loss: 0.0748 - output_1_Top-1 Accuracy: 0.4220 - output_1_Top-5 Accuracy: 0.7002 - val_loss: 2.9487 - val_output_1_loss: 2.7484 - val_output_2_loss: 0.0668 - val_output_1_Top-1 Accuracy: 0.3572 - val_output_1_Top-5 Accuracy: 0.6297 - lr: 1.0000e-03 - 67s/epoch - 86ms/step
Epoch 19/100
782/782 - 73s - loss: 2.5993 - output_1_loss: 2.3746 - output_2_loss: 0.0749 - output_1_Top-1 Accuracy: 0.4227 - output_1_Top-5 Accuracy: 0.7013 - val_loss: 2.9484 - val_output_1_loss: 2.7480 - val_output_2_loss: 0.0668 - val_output_1_Top-1 Accuracy: 0.3571 - val_output_1_Top-5 Accuracy: 0.6313 - lr: 1.0000e-03 - 73s/epoch - 93ms/step
Epoch 20/100

Epoch 20: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.
782/782 - 69s - loss: 2.6022 - output_1_loss: 2.3773 - output_2_loss: 0.0750 - output_1_Top-1 Accuracy: 0.4224 - output_1_Top-5 Accuracy: 0.7020 - val_loss: 2.9491 - val_output_1_loss: 2.7487 - val_output_2_loss: 0.0668 - val_output_1_Top-1 Accuracy: 0.3577 - val_output_1_Top-5 Accuracy: 0.6291 - lr: 1.0000e-03 - 69s/epoch - 88ms/step
Epoch 21/100
782/782 - 69s - loss: 2.5970 - output_1_loss: 2.3721 - output_2_loss: 0.0750 - output_1_Top-1 Accuracy: 0.4232 - output_1_Top-5 Accuracy: 0.7034 - val_loss: 2.9482 - val_output_1_loss: 2.7478 - val_output_2_loss: 0.0668 - val_output_1_Top-1 Accuracy: 0.3565 - val_output_1_Top-5 Accuracy: 0.6292 - lr: 1.0000e-04 - 69s/epoch - 88ms/step
Epoch 22/100
782/782 - 69s - loss: 2.5930 - output_1_loss: 2.3682 - output_2_loss: 0.0749 - output_1_Top-1 Accuracy: 0.4241 - output_1_Top-5 Accuracy: 0.7043 - val_loss: 2.9481 - val_output_1_loss: 2.7477 - val_output_2_loss: 0.0668 - val_output_1_Top-1 Accuracy: 0.3570 - val_output_1_Top-5 Accuracy: 0.6303 - lr: 1.0000e-04 - 69s/epoch - 88ms/step
Epoch 23/100

Epoch 23: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.
782/782 - 66s - loss: 2.5976 - output_1_loss: 2.3728 - output_2_loss: 0.0749 - output_1_Top-1 Accuracy: 0.4239 - output_1_Top-5 Accuracy: 0.7028 - val_loss: 2.9486 - val_output_1_loss: 2.7481 - val_output_2_loss: 0.0669 - val_output_1_Top-1 Accuracy: 0.3563 - val_output_1_Top-5 Accuracy: 0.6298 - lr: 1.0000e-04 - 66s/epoch - 84ms/step
Epoch 24/100
782/782 - 69s - loss: 2.5965 - output_1_loss: 2.3716 - output_2_loss: 0.0750 - output_1_Top-1 Accuracy: 0.4244 - output_1_Top-5 Accuracy: 0.7025 - val_loss: 2.9488 - val_output_1_loss: 2.7481 - val_output_2_loss: 0.0669 - val_output_1_Top-1 Accuracy: 0.3564 - val_output_1_Top-5 Accuracy: 0.6311 - lr: 1.0000e-05 - 69s/epoch - 88ms/step
Epoch 25/100
782/782 - 69s - loss: 2.5929 - output_1_loss: 2.3680 - output_2_loss: 0.0750 - output_1_Top-1 Accuracy: 0.4248 - output_1_Top-5 Accuracy: 0.7033 - val_loss: 2.9482 - val_output_1_loss: 2.7476 - val_output_2_loss: 0.0669 - val_output_1_Top-1 Accuracy: 0.3572 - val_output_1_Top-5 Accuracy: 0.6307 - lr: 1.0000e-05 - 69s/epoch - 88ms/step
Epoch 26/100

Epoch 26: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.
782/782 - 69s - loss: 2.5973 - output_1_loss: 2.3726 - output_2_loss: 0.0749 - output_1_Top-1 Accuracy: 0.4240 - output_1_Top-5 Accuracy: 0.7021 - val_loss: 2.9477 - val_output_1_loss: 2.7473 - val_output_2_loss: 0.0668 - val_output_1_Top-1 Accuracy: 0.3570 - val_output_1_Top-5 Accuracy: 0.6306 - lr: 1.0000e-05 - 69s/epoch - 88ms/step
Epoch 27/100
782/782 - 69s - loss: 2.5920 - output_1_loss: 2.3672 - output_2_loss: 0.0749 - output_1_Top-1 Accuracy: 0.4255 - output_1_Top-5 Accuracy: 0.7041 - val_loss: 2.9494 - val_output_1_loss: 2.7489 - val_output_2_loss: 0.0668 - val_output_1_Top-1 Accuracy: 0.3568 - val_output_1_Top-5 Accuracy: 0.6310 - lr: 1.0000e-06 - 69s/epoch - 88ms/step
Epoch 27: early stopping
2024-05-02 23:35:58.005045: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.
2024-05-02 23:35:58.005119: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.
2024-05-02 23:35:58.005469: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: etinynet_48
2024-05-02 23:35:58.032516: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }
2024-05-02 23:35:58.032594: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: etinynet_48
2024-05-02 23:35:58.079926: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
2024-05-02 23:35:58.100056: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.
2024-05-02 23:35:58.764530: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: etinynet_48
2024-05-02 23:35:58.966765: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 961299 microseconds.
2024-05-02 23:35:59.158456: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
Summary on the non-converted ops:
---------------------------------
 * Accepted dialects: tfl, builtin, func
 * Non-Converted Ops: 85, Total Ops 132, % non-converted = 64.39 %
 * 85 ARITH ops

- arith.constant:   85 occurrences  (f32: 84, i32: 1)



  (f32: 14)
  (f32: 26)
  (f32: 2)
  (f32: 1)
  (f32: 1)
2024-05-02 23:36:03.440120: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.
2024-05-02 23:36:03.440252: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.
2024-05-02 23:36:03.440483: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: etinynet_48
2024-05-02 23:36:03.467853: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }
2024-05-02 23:36:03.467986: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: etinynet_48
2024-05-02 23:36:03.543828: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.
2024-05-02 23:36:04.184054: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: etinynet_48
2024-05-02 23:36:04.383114: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 942634 microseconds.
Summary on the non-converted ops:
---------------------------------
 * Accepted dialects: tfl, builtin, func
 * Non-Converted Ops: 85, Total Ops 132, % non-converted = 64.39 %
 * 85 ARITH ops

- arith.constant:   85 occurrences  (f32: 84, i32: 1)



  (f32: 14)
  (f32: 26)
  (f32: 2)
  (f32: 1)
  (f32: 1)
fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
<keras.src.engine.input_layer.InputLayer object at 0x7f11cc142860>
<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7f11b89cbbb0>
<keras.src.layers.normalization.batch_normalization.BatchNormalization object at 0x7f11b89cb550>
<keras.src.layers.core.activation.Activation object at 0x7f11c25d8f10>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89cb7c0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89849a0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d3c2b940>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8d63610>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89d13f0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b894dfc0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89ea710>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b8975db0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89d5990>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89e5750>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b8916c80>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d3c34610>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89f73a0>
<keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D object at 0x7f11d81afc40>
<keras.src.layers.regularization.dropout.Dropout object at 0x7f11b89e4a90>
<keras.src.layers.core.dense.Dense object at 0x7f11d3c6df30>
<tempature_softmax_activation_layer.TempatureSoftmaxActivationLayer object at 0x7f11d3c2d330>
<keras.src.engine.input_layer.InputLayer object at 0x7f11cc142860>
<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7f11b89cbbb0>
<keras.src.layers.normalization.batch_normalization.BatchNormalization object at 0x7f11b89cb550>
<keras.src.layers.core.activation.Activation object at 0x7f11c25d8f10>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89cb7c0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89849a0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d3c2b940>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8d63610>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89d13f0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b894dfc0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89ea710>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b8975db0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89d5990>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89e5750>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b8916c80>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d3c34610>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89f73a0>
<keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D object at 0x7f11d81afc40>
<keras.src.layers.regularization.dropout.Dropout object at 0x7f11b89e4a90>
<keras.src.layers.core.dense.Dense object at 0x7f11d3c6df30>
<tempature_softmax_activation_layer.TempatureSoftmaxActivationLayer object at 0x7f11d3c2d330>
<keras.src.engine.input_layer.InputLayer object at 0x7f11cc142860>
<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7f11b89cbbb0>
<keras.src.layers.normalization.batch_normalization.BatchNormalization object at 0x7f11b89cb550>
<keras.src.layers.core.activation.Activation object at 0x7f11c25d8f10>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89cb7c0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89849a0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d3c2b940>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8d63610>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89d13f0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b894dfc0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89ea710>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b8975db0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89d5990>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89e5750>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b8916c80>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d3c34610>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89f73a0>
<keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D object at 0x7f11d81afc40>
<keras.src.layers.regularization.dropout.Dropout object at 0x7f11b89e4a90>
<keras.src.layers.core.dense.Dense object at 0x7f11d3c6df30>
<tempature_softmax_activation_layer.TempatureSoftmaxActivationLayer object at 0x7f11d3c2d330>
<keras.src.engine.input_layer.InputLayer object at 0x7f11cc142860>
<keras.src.layers.convolutional.conv2d.Conv2D object at 0x7f11b89cbbb0>
<keras.src.layers.normalization.batch_normalization.BatchNormalization object at 0x7f11b89cb550>
<keras.src.layers.core.activation.Activation object at 0x7f11c25d8f10>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89cb7c0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89849a0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d3c2b940>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d8d63610>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89d13f0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b894dfc0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89ea710>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b8975db0>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89d5990>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89e5750>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b8916c80>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11d3c34610>
<etinynet_blocks.LinearBottleneckBlock object at 0x7f11b89f73a0>
<keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D object at 0x7f11d81afc40>
<keras.src.layers.regularization.dropout.Dropout object at 0x7f11b89e4a90>
<keras.src.layers.core.dense.Dense object at 0x7f11d3c6df30>
<tempature_softmax_activation_layer.TempatureSoftmaxActivationLayer object at 0x7f11d3c2d330>
1592.61328125
479.8125
Traceback (most recent call last):
  File "/home/nathan/Documents/etinynet_cortex_m4/main.py", line 212, in <module>
  File "/home/nathan/Documents/etinynet_cortex_m4/main.py", line 199, in classify_sample_tflite
  File "/home/nathan/Documents/etinynet_cortex_m4/venv/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py", line 88, in wrapper
    return op(*args, **kwargs)
  File "/home/nathan/Documents/etinynet_cortex_m4/venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/nathan/Documents/etinynet_cortex_m4/venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 884736 values, but the requested shape has 6912 [Op:Reshape]
